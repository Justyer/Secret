# coding=gbk
#单例测试导入路径
if __name__ == '__main__':
    import sys, os
    parent_path = os.path.dirname(os.getcwd())
    sys.path.append(parent_path)

from webparser.webparserbase import ParserBase
from serializ.htmlfile import *
from serializ.oracle import *
from bean.urlbean import *
from log.logger import *

LOG=logging.getLogger()
LOG.handlers[0].setLevel(logging.INFO)
LOG.handlers[1].setLevel(logging.DEBUG)

################################
# metadatas = ('题名','来源链接','小区名称','所属区域','小区链接','楼栋名称','地址','建成年份',                  #7
#             '总楼层','当前层','朝向','建筑面积','使用面积','产权性质','装修情况','户型','卧室数量',            #16
#             '客厅数量','卫生间数量','厨房数量','阳台数量','单价','总价','挂牌时间','房屋图片','信息来源',      #25
#             '联系人','经纪公司','电话号码','纬度','经度','发布时间','住宅类别','建筑类别','配套设施','交通状况', #35
#             '楼盘物业类型','楼盘绿化率','楼盘物业费','数据来源','城市','行政区','str_order')                    #41

metadatas = ('信息来源','城市','行政区','片区','来源链接','题名','小区名称','小区链接','地址','建成年份','房龄',                     #10
             '楼层','总楼层','当前层','朝向','建筑面积','使用面积','户型','卧室数量','客厅数量','卫生间数量',                        #20
             '厨房数量','阳台数量','总价','单价','本月均价','小区开盘单价','住宅类别','产权性质','装修情况','建筑类别',               #30
             '楼盘物业类型','小区简介','联系人','联系人链接','经纪公司','门店','电话号码','服务商圈','注册时间','经纪公司房源编号',    #40
             '发布时间','小区总户数','小区总建筑面积','容积率','小区总停车位','开发商','交通状况','配套设施','楼盘绿化率','物业公司',  #50
             '楼盘物业费','土地使用年限','入住率','学校','地上层数','花园面积','地下室面积','车库数量','车位数量','厅结构',           #60
             '导航','房屋图片','纬度','经度','str_order','数据来源')                                                                                      #65

import socket,time
#socket.timeout = 3
import requests, re
from bs4 import BeautifulSoup, Tag
#解析搜房网站
class wwwfangcom(ParserBase):
    htmlwrite = HtmlFile()
    headers = {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.152 Safari/537.36 LBBROWSER', 'Connection': 'keep-alive'}
    shi = re.compile(r'(\d+)室|卧|卧室')
    ting = re.compile(r'(\d+)厅')
    wei = re.compile(r'(\d+)卫')
    chu = re.compile(r'(\d+)厨')
    yangtai = re.compile(r'(\d+)台|阳台')

    def __init__(self):
        super(wwwfangcom, self).__init__()
    def default(self, urlbase):
        LOG.info('搜房默认方法获得城市列表!')
        t_90 = ('北京', '上海')
        t_45 = ('深圳','苏州','杭州')
        t_30 = ('天津')
        t_25 = ('广州')
        t_15 = ('重庆','成都','南京','青岛','厦门','郑州','珠海','武汉','济南','东莞')
        t_7 = ('保定','贵阳','长春','桂林','石家庄','合肥','长沙','昆明','宁波','烟台','沈阳','太原',
               '大连','西安','呼和浩特','哈尔滨','无锡','常州','佛山','福州','海南','惠州','江门',
               '江阴','昆山','廊坊','兰州','洛阳','南昌','南宁','南通','秦皇岛','泉州','三亚','泰州','唐山','潍坊','威海',
               '湘潭','咸阳','徐州','扬州','宜昌','银川','中山',
               '温州','金华','漳州','乌鲁木齐','嘉兴','九江','吉林','淮安','常德', '西宁', '鞍山', '石河子', '大庆', '防城港',
               '衡水','滨州','遂宁','黄石','昌吉','伊犁','阿克苏','庆阳')

        r = requests.get(urlbase.url, headers=self.headers, timeout=(3.05, 3.5))
        LOG.info('访问耗时:%.4f, url:%s', r.elapsed.microseconds/1000000, r.url)
        if(r.status_code != requests.codes.ok):
            LOG.warning('wwwfangcom %s 返回状态:%s', urlbase.url, r.status_code)
            return None
        soup = BeautifulSoup(r.content.decode('gbk'), 'html.parser') #lxml
        citys = soup.find('div', id='c01').find_all('a')
        list = []
        i_order = urlbase.order+'0'
        for i in citys:
            if i.text in t_90:
                i_count = 1
                while i_count <= 90:
                    list.append(UrlBean(i['href']+'/house/h316-i3'+str(i_count)+'-j3100/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/house/h316-i3'+str(i_count)+'-j3100/')
                    i_count += 1
            elif i.text in t_45:
                i_count = 1
                while i_count <= 45:
                    list.append(UrlBean(i['href']+'/house/h316-i3'+str(i_count)+'-j3100/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/house/h316-i3'+str(i_count)+'-j3100/')
                    i_count += 1
            elif i.text in t_30:
                i_count = 1
                while i_count <= 30:
                    list.append(UrlBean(i['href']+'/house/h316-i3'+str(i_count)+'-j3100/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/house/h316-i3'+str(i_count)+'-j3100/')
                    i_count += 1
            elif i.text in t_25:
                i_count = 1
                while i_count <= 25:
                    list.append(UrlBean(i['href']+'/house/h316-i3'+str(i_count)+'-j3100/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/house/h316-i3'+str(i_count)+'-j3100/')
                    i_count += 1
            elif i.text in t_15:
                i_count = 1
                while i_count <= 15:
                    list.append(UrlBean(i['href']+'/house/h316-i3'+str(i_count)+'-j3100/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/house/h316-i3'+str(i_count)+'-j3100/')
                    i_count += 1
            elif i.text in t_7:
                i_count = 1
                while i_count <= 7:
                    list.append(UrlBean(i['href']+'/house/h316-i3'+str(i_count)+'-j3100/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/house/h316-i3'+str(i_count)+'-j3100/')
                    i_count += 1
        return list

    #解析城市二手房列表页
    def getpages(self, urlbase):
        LOG.info('fanggetpages获得城市%s二手房列表信息', urlbase.url)
        r = requests.get(urlbase.url, headers=self.headers, timeout=(3.05, 3.5))
        LOG.info('访问耗时:%.4f, url:%s', r.elapsed.microseconds/1000000, r.url)
        if(r.status_code != requests.codes.ok):
            LOG.warning('wwwfangcom %s 返回状态:%s', urlbase.url, r.status_code)
            return None
        soup = BeautifulSoup(r.content.decode('gbk'), 'html.parser') #lxml
        items = soup.find('div', attrs={'class':'houseList'}).find_all('a', attrs={'title': True})
        host = '/'.join(r.url.split('/')[:3])
        itemIndex = 1
        i_order = urlbase.order+'1'
        list = []
        for item in items:
            if '/chushou/' in item['href']:
                list.append(UrlBean(host+item['href'], self.message('getitem'), key=host+item['href'], param=urlbase.headers, order=i_order))
                LOG.debug('%s第%d页%d项' % (urlbase.headers, urlbase.param, itemIndex))
                itemIndex += 1
        #存在分页信息
        # if urlbase.param and urlbase.param<=self.fetchpage:
        #     #获得下一页地址
        #     nextpage = soup.find('div', id='list_D10_15')
        #     if nextpage:
        #         nextpageurl = nextpage.find('a', id ='PageControl1_hlk_next')
        #         if nextpageurl:
        #             list.append(UrlBean(host+nextpageurl['href'], self.message('getpages'), param=urlbase.param+1, headers=urlbase.headers))
        #             LOG.debug('%s第%d页' % (urlbase.headers, urlbase.param))
        return list;

    #解析详细页面信息
    def getitem(self, urlbase):
        LOG.info('fanggetinfo获得城市%s挂牌详细信息', urlbase.url)
        r = requests.get(urlbase.url, headers=self.headers, timeout=(3.05, 3.5))
        LOG.info('访问耗时:%.4f, url:%s', r.elapsed.microseconds/1000000, r.url)
        self.htmlwrite.save('%s\\%s\\%s' %(self.__class__.__name__, '二手房', urlbase.param), r.url, r.text)
        if(r.status_code != requests.codes.ok):
            LOG.warning('wwwfangcom %s 返回状态:%s', urlbase.url, r.status_code)
            return None
        soup = BeautifulSoup(r.content.decode('gbk','ignore'), 'html.parser') #lxml
        reText = r.text
        lr = {}
        ###############################################
        def _split(str_all, str, index):
            try:
                return _strip(str_all).split(str)[index]
            except Exception:
                return ''

        def _strip(str_info):
            return str_info.strip().replace('\n', '').replace(' ', '').replace('\t', '').replace('\r', '')

        title_bs = soup.find('div',attrs={'class':'title'})
        try:
            lr[metadatas[5]] = _strip(title_bs.h1.string)
        except Exception:
            lr[metadatas[5]] = ''

        lr[metadatas[4]] = r.url

        time_zz = '([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8])))'
        time_regexp = re.compile(time_zz)
        try:
            lr[metadatas[41]] = _strip(title_bs.find(text=time_regexp).string.replace('发布时间：', ''))
        except:
            lr[metadatas[41]] = ''

        city_Bs = soup.find('div', attrs={'class':'bread'})

        if city_Bs is not None:
            city = city_Bs.find_all('a')
            lr[metadatas[61]] = (">".join([_strip(dd.text) for dd in city])).replace('二手房', '').strip()
        try:
            lr[metadatas[1]] = _strip(city[1].string.replace('二手房', ''))
        except:
            lr[metadatas[1]] = ''
        try:
            lr[metadatas[2]] = _strip(city[2].string.replace('二手房', ''))
        except:
            lr[metadatas[2]] = ''

        agentInfo_bs = soup.find('div', attrs={'class':'agentInf'})
        if agentInfo_bs is not None:
            try:
                lr[metadatas[33]] = _strip(agentInfo_bs.find('span', attrs={'id': 'Span3'}).string)
            except Exception:
                lr[metadatas[33]] = ''
            try:
                lr[metadatas[34]] = agentInfo_bs.find('a')['href']
            except:
                lr[metadatas[34]] = ''
            if lr[metadatas[34]] != '':
                try:
                    rjjr = requests.get(lr[metadatas[34]], headers=self.headers, timeout=(3.05, 2.5))
                    reTextjjr = rjjr.text
                except:
                    reTextjjr = ''
                if '<div class="contentwrap">' not in reTextjjr:
                    try:
                        lr[metadatas[35]] = re.findall('公<span class="pl24">司</span>：(.*?)</li>',reTextjjr,re.S)[0]
                    except:
                        lr[metadatas[35]] = ''
                    try:
                        lr[metadatas[36]] = re.findall('<li>门店名称：(.*?)</li>',reTextjjr,re.S)[0]
                    except:
                        lr[metadatas[36]] = ''
                    try:
                        lr[metadatas[38]] = re.findall('服务商圈：(.*?)</li>',reTextjjr,re.S)[0]
                        if '</dd>' in lr[metadatas[38]]:
                            lr[metadatas[38]] = _split(lr[metadatas[38]],'</dd>',0)
                    except:
                        lr[metadatas[38]] = ''
                    try:
                        lr[metadatas[39]] = re.findall('<li>注册时间：(.*?)</li>',reTextjjr,re.S)[0]
                    except:
                        lr[metadatas[39]] = ''
                else:
                    try:
                        lr[metadatas[35]] = re.findall('</b><span class="">(.*?)</span>',reTextjjr,re.S)[0]
                    except:
                        lr[metadatas[35]] = ''
                    try:
                        md = re.findall('<li>所属门店：(.*?)</span>',reTextjjr,re.S)[0]
                        dr = re.compile(r'<[^>]+>',re.S)
                        lr[metadatas[36]] =  _strip(re.sub(dr,'',md))
                    except:
                        lr[metadatas[36]] = ''
                    try:
                        sq = re.findall('<li>服务商圈：(.*?)</span>',reTextjjr,re.S)[0]
                        dr = re.compile(r'<[^>]+>',re.S)
                        lr[metadatas[38]] =  _strip(re.sub(dr,'',sq))
                        if '</dd>' in lr[metadatas[38]]:
                            lr[metadatas[38]] = _split(lr[metadatas[38]],'</dd>',0)
                    except:
                        lr[metadatas[38]] = ''
                    try:
                        rzsj = re.findall('入职时间：(.*?)</li>',reTextjjr,re.S)[0]
                        dr = re.compile(r'<[^>]+>',re.S)
                        lr[metadatas[39]] =  _strip(re.sub(dr,'',rzsj))
                    except:
                        lr[metadatas[39]] = ''
        else:
            lr[metadatas[33]] = lr[metadatas[34]] = lr[metadatas[35]] = lr[metadatas[36]] = lr[metadatas[38]] = lr[metadatas[39]] = ''

        try:
            lr[metadatas[37]] = _strip(soup.find(id="mobilecode").text)
        except:
            lr[metadatas[37]] = ''
        try:
            lr[metadatas[40]] = re.findall('<span class="mr10">房源编号：(.*?)</span>',reText,re.S)[0]
        except:
            lr[metadatas[40]] = ''

        inforTxt_bs = soup.find('div', attrs={'class': 'inforTxt'})
        try:
            lr[metadatas[23]] = _strip(inforTxt_bs.find('span', {'class': 'red20b'}).string)
        except:
            lr[metadatas[23]] = ''
        try:
            lr[metadatas[24]] = _split(inforTxt_bs.dl.dt.text, '(', 1).split(')')[0].replace('元/O', '')
            if lr[metadatas[24]] == '':
                lr[metadatas[24]] = _split(inforTxt_bs.dl.dt.text, '（', 1).split('）')[0].replace('元/O', '')
        except:
            lr[metadatas[24]] = ''
        fit_str = "".join([dd.text for dd in inforTxt_bs.find('dl').find_all('dd')])
        # print('fit_str', fit_str)
        try:
            lr[metadatas[17]] = _split(fit_str, '户型：', 1).split('建筑面积：')[0]
        except:
            lr[metadatas[17]] = ''
        try:
            lr[metadatas[18]] = re.search(wwwfangcom.shi,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[18]] = ''
        try:
            lr[metadatas[19]] = re.search(wwwfangcom.ting,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[19]] = ''
        try:
            lr[metadatas[20]] = re.search(wwwfangcom.wei,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[20]] = ''
        try:
            lr[metadatas[21]] = re.search(wwwfangcom.chu,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[21]] = ''
        try:
            lr[metadatas[22]] = re.search(wwwfangcom.yangtai,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[22]] = ''
        try:
            lr[metadatas[15]] = _split(fit_str, '建筑面积：', 1).split('使用面积：')[0].replace('O', '').replace('O','')
        except:
            lr[metadatas[15]] = ''
        try:
            lr[metadatas[16]] = _split(fit_str, '使用面积：', 1).split('O')[0].replace('O', '').replace('O','')
        except:
            lr[metadatas[16]] = ''
        if '年代' in lr[metadatas[16]]:
            lr[metadatas[16]] = ''
        try:
            str_next_dl = inforTxt_bs.dl.find_next('dl').text
        except Exception as e:
            str_next_dl = ''
        lr[metadatas[9]] = _split(str_next_dl, '年代：', 1).split('年')[0].strip()
        louceng = _split(str_next_dl, '楼层：', 1).split(')')[0].strip()+')'
        lr[metadatas[11]] = louceng
        try:
            lr[metadatas[12]] = re.findall('共(.*?)层',louceng,re.S)[0]
            lr[metadatas[13]] = re.findall('第(.*?)层',louceng,re.S)[0]
        except:
            lr[metadatas[12]] = ''
            lr[metadatas[13]] = ''

        lr[metadatas[30]] = _split(str_next_dl, '建筑类别：', 1).split('产权性质：')[0].strip()
        lr[metadatas[27]] = _split(str_next_dl, '住宅类别：', 1).split('宅')[0].strip()+'宅'
        lr[metadatas[28]] = _split(str_next_dl, '产权性质：', 1).split('楼盘名称：')[0].strip()
        lr[metadatas[29]] = _split(str_next_dl, '装修：', 1).split('住宅类别：')[0].strip()
        if '建筑类别：' in lr[metadatas[29]]:
            lr[metadatas[30]] = _split(lr[metadatas[29]], '装修情况：', 1).split('产权性质：')[0]
            lr[metadatas[28]] = _split(lr[metadatas[29]], '产权性质：', 1).split('楼盘名称：')[0]
            lr[metadatas[29]] = _split(lr[metadatas[29]], '建筑类别：', 0)
        elif '产权性质' in lr[metadatas[29]]:
            lr[metadatas[29]] = _split(lr[metadatas[29]], '产权性质：', 0)
        lr[metadatas[14]] = _split(str_next_dl,'朝向：',1).split('楼层：')[0].strip()
        if '地上层数' in lr[metadatas[14]]:
            lr[metadatas[55]] = _split(lr[metadatas[14]], '地上层数：', 1).split('装修程度：')[0]
            lr[metadatas[56]] = _split(lr[metadatas[14]], '花园面积：', 1).split('车库数量：')[0]
            if '厅结构' in lr[metadatas[56]]:
                lr[metadatas[60]] = lr[metadatas[56]].split('厅结构：')[1].strip()
                if '车位数量：' in lr[metadatas[60]]:
                    lr[metadatas[60]] = _split(lr[metadatas[60]], '车位数量：', 0)
                lr[metadatas[56]] = lr[metadatas[56]].split('厅结构：')[0].strip()
            if '地下室面积：' in lr[metadatas[56]]:
                lr[metadatas[56]] = lr[metadatas[56]].split('地下室面积：')[0].strip()
            lr[metadatas[57]] = _split(lr[metadatas[14]], '地下室面积：', 1).split('楼盘名称：')[0]
            lr[metadatas[58]] = _split(lr[metadatas[14]], '车库数量：', 1).split('车位数量：')[0]
            if '楼盘名称：' in lr[metadatas[58]]:
                lr[metadatas[58]] = lr[metadatas[58]].split('楼盘名称：')[0].strip()
            lr[metadatas[59]] = _split(lr[metadatas[14]], '车位数量：', 1).split('地下室面积：')[0]
            if '楼盘名称：' in lr[metadatas[59]]:
                lr[metadatas[59]] = lr[metadatas[59]].split('楼盘名称：')[0].strip()
            lr[metadatas[29]] = _split(lr[metadatas[14]], '装修程度：', 1).split('建筑形式：')[0]
            lr[metadatas[30]] = _split(lr[metadatas[14]], '建筑形式：', 1).split('面积：')[0]
            if '楼盘名称：' in lr[metadatas[30]]:
                lr[metadatas[30]] = lr[metadatas[30]].split('楼盘名称：')[0].strip()
            lr[metadatas[14]] = lr[metadatas[14]].split('地上层数')[0]
        elif '结构：' in lr[metadatas[14]]:
                lr[metadatas[14]] = lr[metadatas[14]].split('结构')[0]
        elif '装修：' in lr[metadatas[14]]:
                lr[metadatas[14]] = lr[metadatas[14]].split('装修')[0]
        if '楼层' in lr[metadatas[15]]:
            #150楼层：中层（共18层）朝向：南北装修：毛坯年代：2014年
            if '（共' in lr[metadatas[15]]:
                lr[metadatas[12]] = re.findall('（共(.*?)层',lr[metadatas[15]],re.S)[0]
                lr[metadatas[13]] = re.findall('楼层：(.*?)层',lr[metadatas[15]],re.S)[0]
                lr[metadatas[14]] = _split(lr[metadatas[15]], '朝向：', 1).split('装修：')[0]
                lr[metadatas[29]] = _split(lr[metadatas[15]], '装修：', 1).split('年代：')[0]
                lr[metadatas[9]] = _split(lr[metadatas[15]], '年代：', 1).replace('年','')
            lr[metadatas[15]] = lr[metadatas[15]].split('楼层')[0]

        lr[metadatas[28]] = _split(str_next_dl, '产权性质：', 1).split('楼盘名称：')[0].strip()
        lr[metadatas[6]] = _split(str_next_dl, '楼盘名称：', 1).split('(')[0].strip() \
                           or ''.join(s.strip() for s in re.findall('小区：[^>]*>([^<]*)<', str(inforTxt_bs))) \
                           or _split(str_next_dl, '楼盘名称：', 1).replace("[地图交通]", '')

        try:
            lr[metadatas[3]] = _strip(city[3].string.replace('二手房', ''))
        except:
            lr[metadatas[3]] = ''
        lr[metadatas[48]] = _split(str_next_dl, '配套设施：', 1).strip()
        try:
            xqUrl_Bs = inforTxt_bs.dl.find_next('dl').find_all('a')
        except:
            xqUrl_Bs = ''
        try:
            lr[metadatas[7]] = _strip(xqUrl_Bs[0]['href'])
        except:
            lr[metadatas[7]] = ''
        try:
            xq_Bs = soup.find('div',attrs={'class': 'traffic mt10'})
        except:
            xq_Bs = ''
        try:
            lr[metadatas[8]] = _strip(xq_Bs.find('p').text.replace('地址：', ''))
        except Exception:
            lr[metadatas[8]] = ''
        try:
            lr[metadatas[47]] = _strip(xq_Bs.find('p').find_next('p').text.replace('交通状况：', ''))
        except:
            lr[metadatas[47]] = ''
        try:
            xqjj_Bs = soup.find('div', attrs={'class': 'introduct mt10'})
        except:
            xqjj_Bs = ''
        try:
            byjj = xqjj_Bs.find('span',class_='red20b').text
            lr[metadatas[25]] = _strip(byjj)
        except:
            lr[metadatas[25]] = ''
        try:
            all_xq_dd = xqjj_Bs.find_all('dd')
        except:
            all_xq_dd =[]

        try:
            lr[metadatas[31]] = _strip(all_xq_dd[2].text.strip().replace('物业类型：', ''))
        except:
            lr[metadatas[31]] = ''
        try:
            lr[metadatas[49]] = _strip(all_xq_dd[3].text.strip().replace('绿 化 率：', ''))
        except:
            lr[metadatas[49]] = ''
        try:
            lr[metadatas[51]] = _strip(all_xq_dd[4].text.strip().replace('物 业 费：', ''))
        except:
            lr[metadatas[51]] = ''
        try:
            lr[metadatas[50]] = _strip(all_xq_dd[5].text.strip().replace('物业公司：', ''))
        except:
            lr[metadatas[50]] = ''
        try:
            lr[metadatas[46]] = _strip(all_xq_dd[6].text.strip().replace('开 发 商：', ''))
        except:
            lr[metadatas[46]] = ''

        lr[metadatas[0]] = 's00000sf'
        lr[metadatas[66]] = 'sf'
        lr[metadatas[65]] = urlbase.order if urlbase.order is not None else ''

        lr[metadatas[10]] = ''
        lr[metadatas[26]] = ''
        lr[metadatas[32]] = ''
        lr[metadatas[42]] = ''
        lr[metadatas[43]] = ''
        lr[metadatas[44]] = ''
        lr[metadatas[45]] = ''
        lr[metadatas[52]] = ''
        lr[metadatas[53]] = ''
        lr[metadatas[54]] = ''
        lr[metadatas[62]] = ''
        lr[metadatas[63]] = ''
        lr[metadatas[64]] = ''
        for iitem in metadatas:
            try:
                lr[iitem]
            except:
                lr[iitem] = ''
        # print(metadatas[1], lr[metadatas[1]], '|',
        #       metadatas[2], lr[metadatas[2]], '|',
        #       metadatas[3], lr[metadatas[3]], '|',
        #       metadatas[6], lr[metadatas[6]], '|',
        #       metadatas[15], lr[metadatas[15]], '|',
        #       metadatas[23], lr[metadatas[23]], '|',
        #       metadatas[24], lr[metadatas[24]])
        ###############################################
        #将分析的信息写入数据库
        # self.mysql.save(lr, metadatas)
        MySqlEx.save(lr, metadatas)
        time.sleep(1)
    #def getinfo(self, urlbase):
        #LOG.info('fang8getinfo获得%s挂牌详细信息' , urlbase.url)


if __name__ == '__main__':
    wfang = wwwfangcom()
    #print(len(wfang.default(UrlBase('http://esf.hz.fang.com/newsecond/esfcities.aspx', 'wwwfangcom',order='123'))))
    # wfang.getpages(UrlBean('http://esf.hrb.fang.com/house/h31-i34/', 'wwwsfcom#getitem', param=10, headers='哈尔滨',order='123'))
    # print(w58.getpages(UrlBean('http://bj.58.com/ershoufang/pn2', 'www58com#getitem', param=9, headers='北京')))
    wfang.getitem(UrlBean('http://esf.daqing.fang.com/chushou/14_208088.htm', 'wwwfangcom#getitem', param='北京',order='1234'))
    wfang.getitem(UrlBean('http://esf.guilin.fang.com/chushou/14_207033.htm', 'wwwfangcom#getitem', param='北京',order='1234'))
    ls = ('http://esf.daqing.fang.com/chushou/14_208093.htm',
'http://esf.anshan.fang.com/chushou/14_207037.htm',
'http://esf.hs.fang.com/chushou/14_205360.htm',
'http://esf.changji.fang.com/chushou/14_202450.htm',
'http://esf.qingyang.fang.com/chushou/14_201688.htm',
'http://esf.binzhou.fang.com/chushou/14_204355.htm',
'http://esf.huangshi.fang.com/chushou/14_203025.htm',
'http://esf.yili.fang.com/chushou/14_201547.htm',
'http://esf.huaian.fang.com/chushou/14_207660.htm',
'http://esf.fangchenggang.fang.com/chushou/14_201809.htm',
'http://esf.suining.fang.com/chushou/14_218656.htm',
'http://esf.jl.fang.com/chushou/14_207439.htm',
'http://esf.xt.fang.com/chushou/14_202341.htm',
'http://esf.xn.fang.com/chushou/14_206414.htm',
'http://esf.fang.com/chushou/15_1019408.htm',
'http://esf.jy.fang.com/chushou/14_201649.htm',
'http://esf.akesu.fang.com/chushou/14_201194.htm',
'http://esf.sh.fang.com/chushou/14_852876.htm',
'http://esf.suzhou.fang.com/chushou/14_410748.htm',
'http://esf.jx.fang.com/chushou/3_222630429.htm',
'http://esf.cq.fang.com/chushou/15_1032633.htm',
'http://esf.nanjing.fang.com/chushou/14_394848.htm',
'http://esf.shihezi.fang.com/chushou/14_201394.htm',
'http://esf.xj.fang.com/chushou/14_216384.htm',
'http://esf.tj.fang.com/chushou/14_533422.htm',
'http://esf.gz.fang.com/chushou/15_1032582.htm',
'http://esf.nanping.fang.com/cs/273337.htm',
'http://esf.changde.fang.com/chushou/14_202576.htm',
'http://esf.jiujiang.fang.com/chushou/14_206726.htm',
'http://esf.wuhan.fang.com/chushou/14_368736.htm',
'http://esf.ly.fang.com/chushou/14_210078.htm',
'http://esf.zz.fang.com/chushou/14_322440.htm',
'http://esf.lf.fang.com/chushou/14_213838.htm',
'http://esf.ts.fang.com/chushou/14_216838.htm',
'http://esf.sz.fang.com/chushou/14_430440.htm',
'http://esf.xian.fang.com/chushou/14_323010.htm',
'http://esf.jn.fang.com/chushou/14_334708.htm',
'http://esf.zh.fang.com/chushou/14_247446.htm',
'http://esf.xianyang.fang.com/chushou/14_203364.htm',
'http://esf.dg.fang.com/chushou/14_260010.htm',
'http://esf.nc.fang.com/chushou/14_300632.htm',
'http://esf.sy.fang.com/chushou/14_331448.htm',
'http://esf.yanbian.fang.com/cs/175050.htm',
'http://esf.jm.fang.com/chushou/14_208518.htm',
'http://esf.wuxi.fang.com/chushou/14_271941.htm',
'http://esf.cd.fang.com/chushou/14_537119.htm',
'http://esf.qd.fang.com/chushou/14_300015.htm',
'http://esf.zhangzhou.fang.com/chushou/14_203786.htm',
'http://esf.xz.fang.com/chushou/14_216122.htm',
'http://esf.xm.fang.com/chushou/14_217834.htm',
'http://esf.qhd.fang.com/chushou/14_208500.htm',
'http://esf.yinchuan.fang.com/chushou/14_209735.htm',
'http://esf.sjz.fang.com/chushou/14_300514.htm',
'http://esf.fz.fang.com/chushou/14_241042.htm',
'http://esf.weihai.fang.com/chushou/14_210867.htm',
'http://esf.lijiang.fang.com/cs/142586.htm',
'http://esf.taizhou.fang.com/chushou/14_207118.htm',
'http://esf.jh.fang.com/chushou/14_204026.htm',
'http://esf.nm.fang.com/chushou/14_215711.htm',
'http://esf.chuzhou.fang.com/cs/104961.htm',
'http://esf.changchun.fang.com/chushou/14_328152.htm',
'http://esf.cs.fang.com/chushou/14_264240.htm',
'http://esf.longyan.fang.com/cs/493000.htm',
'http://esf.qz.fang.com/chushou/14_206113.htm',
'http://esf.yc.fang.com/chushou/14_206939.htm',
'http://esf.hz.fang.com/chushou/14_419108.htm',
'http://esf.nn.fang.com/chushou/14_256485.htm',
'http://esf.baoshan.fang.com/cs/99616.htm',
'http://esf.km.fang.com/chushou/14_280454.htm',
'http://esf.wz.fang.com/chushou/14_204529.htm',
'http://esf.guangyuan.fang.com/cs/150827.htm',
'http://esf.zhaoqing.fang.com/cs/322731.htm',
'http://esf.yancheng.fang.com/cs/24031.htm',
'http://esf.bazhong.fang.com/cs/182801.htm',
'http://esf.hengyang.fang.com/cs/45168.htm',
'http://esf.hechi.fang.com/cs/90027.htm',
'http://esf.heihe.fang.com/cs/59914.htm',
'http://esf.dy.fang.com/cs/339584.htm',
'http://esf.shanwei.fang.com/cs/181978.htm',
'http://esf.maoming.fang.com/cs/315174.htm',
'http://esf.byne.fang.com/cs/17001.htm',
'http://esf.puyang.fang.com/cs/199576.htm',
'http://esf.zhuzhou.fang.com/cs/49304.htm',
'http://esf.changshu.fang.com/cs/775.htm',
'http://esf.jining.fang.com/cs/112084.htm',
'http://esf.dz.fang.com/cs/17079.htm',
'http://esf.dl.fang.com/chushou/14_330604.htm',
'http://esf.xuchang.fang.com/cs/228345.htm',
'http://esf.jixi.fang.com/cs/129263.htm',
'http://esf.zj.fang.com/cs/39579.htm',
'http://esf.luohe.fang.com/cs/260622.htm',
'http://esf.huzhou.fang.com/cs/7127.htm',
'http://esf.nanchong.fang.com/cs/57138.htm',
'http://esf.zhenjiang.fang.com/cs/49648.htm',
'http://esf.wuhu.fang.com/cs/56753.htm',
'http://esf.sanming.fang.com/cs/199685.htm',
'http://esf.dali.fang.com/cs/246892.htm',
'http://esf.songyuan.fang.com/cs/112068.htm',
'http://esf.yangjiang.fang.com/cs/237338.htm',
'http://esf.heze.fang.com/cs/260096.htm',
'http://esf.xx.fang.com/cs/83081.htm',
'http://esf.mianyang.fang.com/cs/25970.htm',
'http://esf.ankang.fang.com/cs/356914.htm',
'http://esf.pingdingshan.fang.com/cs/78543.htm',
'http://esf.wf.fang.com/chushou/14_220178.htm',
'http://esf.lyg.fang.com/cs/92168.htm',
'http://esf.xishuangbanna.fang.com/cs/98518.htm',
'http://esf.zhumadian.fang.com/cs/107654.htm',
'http://esf.taian.fang.com/cs/91765.htm',
'http://esf.guilin.fang.com/chushou/14_207033.htm',
'http://esf.zb.fang.com/cs/105183170.htm',
'http://esf.wanzhou.fang.com/cs/762.htm',
'http://esf.st.fang.com/cs/11537.htm',
'http://esf.yuncheng.fang.com/cs/123559.htm',
'http://esf.liuzhou.fang.com/cs/10518.htm',
'http://esf.meizhou.fang.com/cs/95255.htm',
'http://esf.huainan.fang.com/cs/207219.htm',
'http://esf.anyang.fang.com/cs/59040.htm',
'http://esf.nanan.fang.com/cs/2446.htm',
'http://esf.linyi.fang.com/cs/116503.htm',
'http://esf.bengbu.fang.com/cs/42178.htm',
'http://esf.tc.fang.com/cs/4837.htm',
'http://esf.shaoguan.fang.com/cs/125742.htm',
'http://esf.xingtai.fang.com/cs/217665.htm',
'http://esf.tengzhou.fang.com/cs/17681.htm',
'http://esf.sq.fang.com/cs/177405.htm',
'http://esf.fuyang.fang.com/cs/244977.htm',
'http://esf.nanyang.fang.com/cs/157363.htm',
'http://esf.zoucheng.fang.com/cs/10170.htm',
'http://esf.puer.fang.com/cs/114129.htm',
'http://esf.liuyang.fang.com/cs/73.htm',
'http://esf.shuyang.fang.com/cs/5960.htm',
'http://esf.rugao.fang.com/cs/14266.htm',
'http://esf.macau.fang.com/cs/155.htm'
)
    for l in ls:
        wfang.getitem(UrlBean(l, 'wwwfangcom#getitem', param='北京',order='1234'))
#print(www58com.__name__)
#print(www58com.getcitys.__name__)
#print(isinstance([], list))

"""
print(www58com.__name__)
#print(sys.modules.keys())
print(sys.modules['__main__'])

import sys
import os

def get_module():

    def main_module_name():
        mod = sys.modules['__main__']
        file = getattr(mod, '__file__', None)
        return file and os.path.splitext(os.path.basename(file))[0]

    def modname(fvars):

        file, name = fvars.get('__file__'), fvars.get('__name__')
        if file is None or name is None:
            return None

        if name == '__main__':
            name = main_module_name()
        return name

    module_name = modname(globals())
    # print globals()
    # print module_name

get_module()
"""