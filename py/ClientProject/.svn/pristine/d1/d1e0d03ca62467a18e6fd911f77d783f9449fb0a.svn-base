#单例测试导入路径
if __name__ == '__main__':
    import sys, os
    parent_path = os.path.dirname(os.getcwd())
    sys.path.append(parent_path)

from webparser.webparserbase import ParserBase
from serializ.htmlfile import *
from serializ.oracle import *
from bean.urlbean import *
from log.logger import *

LOG=logging.getLogger()
LOG.handlers[0].setLevel(logging.INFO)
LOG.handlers[1].setLevel(logging.INFO)

################################
# metadatas = ('题名','来源链接','小区名称','所属区域','小区链接','楼栋名称','地址','建成年份',                  #7
#             '总楼层','当前层','朝向','建筑面积','使用面积','产权性质','装修情况','户型','卧室数量',            #16
#             '客厅数量','卫生间数量','厨房数量','阳台数量','单价','总价','挂牌时间','房屋图片','信息来源',      #25
#             '联系人','经纪公司','电话号码','纬度','经度','发布时间','住宅类别','建筑类别','配套设施','交通状况', #35
#             '楼盘物业类型','楼盘绿化率','楼盘物业费','数据来源','城市','行政区','str_order')                    #41

metadatas = ('信息来源','城市','行政区','片区','来源链接','题名','小区名称','小区链接','地址','建成年份','房龄',                     #10
             '楼层','总楼层','当前层','朝向','建筑面积','使用面积','户型','卧室数量','客厅数量','卫生间数量',                        #20
             '厨房数量','阳台数量','总价','单价','本月均价','小区开盘单价','住宅类别','产权性质','装修情况','建筑类别',               #30
             '楼盘物业类型','小区简介','联系人','联系人链接','经纪公司','门店','电话号码','服务商圈','注册时间','经纪公司房源编号',    #40
             '发布时间','小区总户数','小区总建筑面积','容积率','小区总停车位','开发商','交通状况','配套设施','楼盘绿化率','物业公司',  #50
             '楼盘物业费','土地使用年限','入住率','学校','地上层数','花园面积','地下室面积','车库数量','车位数量','厅结构',           #60
             '导航','房屋图片','纬度','经度','str_order','数据来源')                                                                                       #65

import requests, re
from bs4 import BeautifulSoup, Tag
#解析安居客网站
class wwwajkcom(ParserBase):
    headers = {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.152 Safari/537.36 LBBROWSER', 'Connection': 'keep-alive'}
    htmlwrite = HtmlFile()
    shi = re.compile(r'(\d+)室|卧|卧室')
    ting = re.compile(r'(\d+)厅')
    wei = re.compile(r'(\d+)卫')
    chu = re.compile(r'(\d+)厨')
    yangtai = re.compile(r'(\d+)台|阳台')

    def __init__(self):
        super(wwwajkcom, self).__init__()

    def default(self, urlbase):
        LOG.info('安居客默认方法获得城市列表!')
        r = requests.get(urlbase.url,headers=self.headers, timeout=(3.05, 1.5))
        LOG.info('访问耗时:%.4f, url:%s', r.elapsed.microseconds/1000000, r.url)
        if(r.status_code != requests.codes.ok):
            LOG.warning('wwwajkcom %s 返回状态:%s', urlbase.url, r.status_code)
            return None
        soup = BeautifulSoup(r.content.decode('utf8'), 'html.parser') #lxml
        citys = soup.find('div', class_='left_side').find_all('a')
        citys.extend(soup.find('div', class_='right_side').find_all('a'))

        list = []
        i_count = 1
        i_order = urlbase.order+'0'
        t_70 = ('上海')
        t_40 = ('北京')
        t_30 = ('深圳', '杭州')
        t_20 = ('天津', '厦门', '成都', '广州', '重庆')
        t_15 = ('南京','昆山','昆明','沈阳', '石家庄', '长沙', '济南', '苏州', '武汉','合肥')
        t_10 = ('郑州','廊坊','南宁','保定','东莞','西安','青岛')
        t_6 = ('烟台','温州','福州','海口','大连','哈尔滨','长春','太原','乌鲁木齐','兰州','西宁','南昌','珠海','大同','连云港','潍坊', '惠州')
        for i in citys:
            #if i.text not in ('北京', '上海', '广州', '深圳'): continue
            if i.text in t_70:
                i_count = 1
                while i_count <= 70:
                    list.append(UrlBean(i['href']+'/sale/o5-p'+str(i_count)+'/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p'+str(i_count)+'/')
                    i_count += 1
            elif i.text in t_40:
                i_count = 1
                while i_count <= 40:
                    list.append(UrlBean(i['href']+'/sale/o5-p'+str(i_count)+'/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p'+str(i_count)+'/')
                    i_count += 1
            elif i.text in t_30:
                i_count = 1
                while i_count <= 30:
                    list.append(UrlBean(i['href']+'/sale/o5-p'+str(i_count)+'/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p'+str(i_count)+'/')
                    i_count += 1
            elif i.text in t_20:
                i_count = 1
                while i_count <= 20:
                    list.append(UrlBean(i['href']+'/sale/o5-p'+str(i_count)+'/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p'+str(i_count)+'/')
                    i_count += 1
            elif i.text in t_15:
                i_count = 1
                while i_count <= 15:
                    list.append(UrlBean(i['href']+'/sale/o5-p'+str(i_count)+'/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p'+str(i_count)+'/')
                    i_count += 1
            elif i.text in t_10:
                i_count = 1
                while i_count <= 10:
                    list.append(UrlBean(i['href']+'/sale/o5-p'+str(i_count)+'/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p'+str(i_count)+'/')
                    i_count += 1
            elif i.text in t_6:
                i_count = 1
                while i_count <= 6:
                    list.append(UrlBean(i['href']+'/sale/o5-p'+str(i_count)+'/', self.message('getpages'), param=i_count, headers=i.text, order=i_order))
                    LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p'+str(i_count)+'/')
                    i_count += 1
            else:
                list.append(UrlBean(i['href']+'/sale/o5-p1/', self.message('getpages'), param=1, headers=i.text, order=i_order))
                LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p1/')
                list.append(UrlBean(i['href']+'/sale/o5-p2/', self.message('getpages'), param=2, headers=i.text, order=i_order))
                LOG.info('获得城市%s页面信息%s', i.text, i['href']+'/sale/o5-p2/')
        return list

    #解析城市二手房列表页
    def getpages(self, urlbase):
        LOG.info('ajkgetpages获得城市%s二手房列表信息', urlbase.url)
        r = requests.get(urlbase.url,headers=self.headers, timeout=(3.05, 1.5))
        LOG.info('访问耗时:%.4f, url:%s', r.elapsed.microseconds/1000000, r.url)
        if(r.status_code != requests.codes.ok):
            LOG.warning('wwwajkcom %s 返回状态:%s', urlbase.url, r.status_code)
            return None
        html = r.text.encode('utf8',errors='ignore').decode('utf8',errors='ignore')
        htmllist = '<ul id="house-list" class="house-list">'+html.split('<ul id="house-list" class="house-list">')[1].split('<div id="IFX_p937" class="" style=""></div>')[0]
        soup = BeautifulSoup(htmllist, 'html.parser')
        items = soup.find_all('a')
        list = []
        itemIndex = 1
        i_order = urlbase.order+'1'
        for item in items:
            if 'prop/view' in item['href']: #and '.58.com' not in item['href'] and 'fang_shou' not in item['href'] and 'fang_zu' not in item['href']:
                sUrl = item['href'].split('?')[0]
                list.append(UrlBean(sUrl, self.message('getitem'), key=sUrl, param=urlbase.headers, order=i_order))
                LOG.debug('%s第%d页%d项' % (urlbase.headers, urlbase.param, itemIndex))
                itemIndex += 1
        #存在分页信息
        # if urlbase.param and urlbase.param<=self.fetchpage:
        #     #获得下一页地址
        #     nextpage = soup.find('div', id='house-area')
        #     if nextpage:
        #         nextpageurl = nextpage.find('a', class_='next')
        #         if nextpageurl:
        #             host = '/'.join(r.url.split('/')[:3])
        #             list.append(UrlBean(host+nextpageurl['href'], self.message('getpages'), param=urlbase.param+1, headers=urlbase.headers))
        #             LOG.debug('%s第%d页' % (urlbase.headers, urlbase.param))
        return list;

    #解析详细页面信息
    def getitem(self, urlbase):
        LOG.info('ajkgetinfo获得城市%s挂牌详细信息', urlbase.url)
        t1 = time.time()
        r = requests.get(urlbase.url,headers=self.headers, timeout=(3.05, 1.5))
        t2 = time.time()
        LOG.info('处理ajk请求getitem耗时:%f' % (t2-t1))
        LOG.info('访问耗时:%.4f, url:%s', r.elapsed.microseconds/1000000, r.url)
        #存储页面信息
        t1 = time.time()
        self.htmlwrite.save('%s\\%s\\%s' %(self.__class__.__name__, '二手房', urlbase.param), r.url, r.text)
        t2 = time.time()
        if(r.status_code != requests.codes.ok):
            LOG.warning('wwwajkom %s 返回状态:%s', r.request.url, r.status_code)
            return
        t1 = time.time()
        reText = r.text
        soup = BeautifulSoup(r.content.decode('utf8'), 'html.parser') #lxml
        lr = {}
        ###############################################################################################################
        def _split(str_all, str, index):
            try:
                return _strip(str_all).split(str)[index]
            except Exception:
                return ''

        def _strip(str_info):
            return str_info.strip().replace('\n', '').replace(' ', '').replace('\t', '').replace('\r', '')

        try:
            title = re.findall('<h3 class="fl">(.*?)</h3>',reText,re.S)
            lr[metadatas[5]]  = title[0]
        except Exception as e:
            lr[metadatas[5]]  =''
    
        lr[metadatas[4]] = urlbase.url
    
        try:
            xq = re.findall('小区名</dt>(.*?)</a>',reText,re.S)
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[6]] = dr.sub('',xq[0]).strip()
        except:
            lr[metadatas[6]] =''
    
        try:
            lr[metadatas[3]] =''
            area1 = soup.find('div',attrs={'class':'block-area'}).find_all('a')
            for area2 in area1:
                 lr[metadatas[3]] = area2.text.replace('房价','')
        except:
            lr[metadatas[3]] =''
    
        try:
            xq = re.findall('小区名</dt>(.*?)target="_blank">',reText,re.S)
            xq_link = re.findall('<a href="(.*?)"',str(xq),re.S)
            lr[metadatas[7]] = xq_link[0]
        except:
            lr[metadatas[7]] =''
    
        try:
            ld_name = re.findall('<div id="content">(.*?)</div>',reText,re.S)
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[61]] = dr.sub('',ld_name[0]).replace('&gt;','>').replace('二手房','').strip()
        except:
            lr[metadatas[61]] =''
    
        try:
            address = re.findall('地址</dt>(.*?)<a href',reText,re.S)
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[8]] = dr.sub('',address[0]).strip().replace('\n')
        except:
            lr[metadatas[8]] = ''
    
        try:
            year = re.findall('建造年代</dt>(.*?)</dd>',reText,re.S)
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[9]] = dr.sub('',year[0]).strip()
        except:
            lr[metadatas[9]] =''

        try:
            z_floor = re.findall('楼层</dt>(.*?)</dl>',reText,re.S)[0]
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[11]] = dr.sub('',z_floor).strip()
        except:
            lr[metadatas[11]] =''
    
        try:
            z_floor = re.findall('楼层</dt>(.*?)</dl>',reText,re.S)
            floor = re.findall('/(.*?)<',str(z_floor),re.S)
            lr[metadatas[12]] = floor[0]
        except:
            lr[metadatas[12]] =''
    
        try:
            dq_floor = re.findall('<dd>(.*?)/',str(z_floor),re.S)
            lr[metadatas[13]] = dq_floor[0]
        except:
            lr[metadatas[13]] =''
    
        try:
            toward_t = re.findall('朝向</dt>(.*?)</dl>',reText,re.S)
            toward = re.findall('<dd>(.*?)</dd>',str(toward_t),re.S)
            lr[metadatas[14]] = toward[0]
        except:
            lr[metadatas[14]] =''
    
        try:
            jz_area = re.findall('面积</dt>(.*?)</dl>',reText,re.S)
            area = re.findall('<dd>(.*?)平米',str(jz_area),re.S)
            lr[metadatas[15]] = area[0]
        except:
            lr[metadatas[15]] =''
    
        try:
            decorate_t = re.findall('装修</dt>(.*?)</dl>',reText,re.S)
            decorate = re.findall('<dd>(.*?)</dd>',str(decorate_t),re.S)
            lr[metadatas[29]] = decorate[0]
        except:
            lr[metadatas[29]] =''
    
        try:
            huose_t = re.findall('房型</dt>(.*?)</dl>',reText,re.S)
            huose = re.findall('<dd>(.*?)</dd>',str(huose_t),re.S)
            lr[metadatas[17]] = huose[0]
        except:
            lr[metadatas[17]] =''
        try:
            lr[metadatas[18]] = re.search(wwwajkcom.shi,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[18]] = ''
        try:
            lr[metadatas[19]] = re.search(wwwajkcom.ting,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[19]] = ''
        try:
            lr[metadatas[20]] = re.search(wwwajkcom.wei,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[20]] = ''
        try:
            lr[metadatas[21]] = re.search(wwwajkcom.chu,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[21]] = ''
        try:
            lr[metadatas[22]] = re.search(wwwajkcom.yangtai,lr[metadatas[17]]).group(1)
        except:
            lr[metadatas[22]] = ''
    
        try:
            u_price_t = re.findall('单价</dt>(.*?)</dl>',reText,re.S)
            u_price = re.findall('<dd>(.*?)元',str(u_price_t),re.S)
            lr[metadatas[24]] = u_price[0]
        except:
            lr[metadatas[24]] =''
    
        try:
            t_price_t = re.findall('售价</dt>(.*?)</dl>',reText,re.S)
            t_price = re.findall('<strong><span class="f26">(.*?)<',str(t_price_t),re.S)
            lr[metadatas[23]] = t_price[0]
        except:
            lr[metadatas[23]] =''
    
        lr[metadatas[0]] = 's00000aj'
        lr[metadatas[66]] = 'ajk'
    
        try:
            lxr_name = re.findall('<strong class="name">(.*?)<',reText,re.S)
            lr[metadatas[33]] = lxr_name[0]
        except:
            lr[metadatas[33]] =''

        try:
            lr[metadatas[34]] = soup.find('div',class_='broker_more_box').find('a')['href']
        except:
            lr[metadatas[34]] = ''
    
        try:
            company = re.findall('<p class="comp_info">(.*?)</a>',reText,re.S)
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[35]] = dr.sub('',company[0]).strip()
        except:
            lr[metadatas[35]] =''

        try:
            lr[metadatas[36]] = soup.find('div',class_='broker_name').find_all('a')[1]['title']
        except:
            lr[metadatas[36]] = ''
    
        try:
            telephone = re.findall('<div class="broker_icon broker_tel dark_grey"><i class="p_icon icon_tel"></i>(.*?)</div>',reText,re.S)
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[37]] = dr.sub('',telephone[0]).strip()
        except:
            lr[metadatas[37]] =''
    
        try:
            lng = re.findall('\'comm_lng\' : \'(.*?)\'',reText,re.S)
            lr[metadatas[63]] = lng[0]
        except:
            lr[metadatas[63]] =''
    
        try:
            lat = re.findall('\'comm_lat\' : \'(.*?)\'',reText,re.S)
            lr[metadatas[64]] = lat[0]
        except:
            lr[metadatas[64]] =''

        try:
            fybh= re.findall('<div class="text-mute extra-info">房源编号：(.*?)，',reText,re.S)[0]
            lr[metadatas[40]] = fybh
        except:
            lr[metadatas[40]] =''
    
        try:
            fb_time = re.findall('，发布时间：(.*?)</div>',reText,re.S)[0]
            lr[metadatas[41]] = fb_time
        except:
            lr[metadatas[41]] =''

        try:
            zhs = re.findall('总户数</dt>(.*?)</dd>',reText,re.S)[0]
            lr[metadatas[42]] = _strip(zhs.replace('<dd>',''))
        except:
            lr[metadatas[42]] = ''

        try:
            zjmj = re.findall('总建面积</dt>(.*?)</dd>',reText,re.S)[0]
            lr[metadatas[43]] = _split(zjmj.replace('<dd>',''),'平方米',0)
        except:
            lr[metadatas[43]] = ''

        try:
            rjl = re.findall('容积率</dt>(.*?)</dd>',reText,re.S)[0]
            lr[metadatas[44]] = _strip(rjl.replace('<dd>',''))
        except:
            lr[metadatas[44]] = ''

        try:
            tcw = re.findall('停车位</dt>(.*?)</dd>',reText,re.S)[0]
            lr[metadatas[45]] = _strip(tcw.replace('<dd>',''))
        except:
            lr[metadatas[45]] = ''

        try:
            kfs = re.findall('开发商</dt>(.*?)</dd>',reText,re.S)[0]
            lr[metadatas[46]] = _strip(kfs.replace('<dd>',''))
        except:
            lr[metadatas[46]] = ''

        try:
            jtzk = re.findall('交通优势：</span></strong></p>(.*?)</span></p>',reText,re.S)[0]
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[47]] = dr.sub('',jtzk).strip()
        except:
            lr[metadatas[47]] = ''


        try:
            xx = re.findall('学校：(.*?)</span></p>',reText,re.S)[0]
            dr = re.compile(r'<[^>]+>',re.S)
            lr[metadatas[54]] = dr.sub('',xx).strip()[0:200]
        except:
            lr[metadatas[54]] = ''

        try:
            wygs = re.findall('物业公司</dt>(.*?)</dd>',reText,re.S)[0]
            lr[metadatas[50]] = _strip(wygs.replace('<dd>',''))
        except:
            lr[metadatas[50]] = ''
    
        try:
            zz_category = re.findall('类型</dt>(.*?)</dl>',reText,re.S)
            category1 = re.findall('<dd>(.*?)</dd>',str(zz_category),re.S)
            category = category1[1]
            lr[metadatas[27]] = category.replace("\n","").strip()
        except:
             lr[metadatas[27]] =''

    
        try:
            lpwylx_t = re.findall('物业类型</dt>(.*?)</dl>',reText,re.S)
            lpwylx = re.findall('<dd>(.*?)</dd>',str(lpwylx_t),re.S)
            lr[metadatas[31]] = lpwylx[0]
        except:
            lr[metadatas[31]] =''
    
        try:
            greening = re.findall('绿化率</dt>(.*?)</dl>',reText,re.S)
            green = re.findall('<dd>(.*?)</dd>',str(greening),re.S)
            lr[metadatas[49]] = green[0]
        except:
            lr[metadatas[49]] =''
    
        try:
            costs_t = re.findall('物业费用</dt>(.*?)</dl>',reText,re.S)
            costs = re.findall('<dd>(.*?)</dd>',str(costs_t),re.S)
            lr[metadatas[51]] = costs[0]
        except:
            lr[metadatas[51]] =''
    

    
        try:
            city = re.findall('<span class="city">(.*?)<',reText,re.S)
            lr[metadatas[1]] =city[0]
        except:
            lr[metadatas[1]] =''
    
    
        try:
            xz_area1 = re.findall('位置</dt>(.*?)/a>',reText,re.S)
            xz_area = re.findall('">(.*?)<',str(xz_area1),re.S)
            lr[metadatas[2]] = xz_area[0]
        except:
            lr[metadatas[2]] =''
        lr[metadatas[65]] = urlbase.order if urlbase.order is not None else ''

        lr[metadatas[10]] = ''
        lr[metadatas[16]] = ''
        lr[metadatas[25]] = ''
        lr[metadatas[26]] = ''
        lr[metadatas[28]] = ''
        lr[metadatas[30]] = ''
        lr[metadatas[32]] = ''
        lr[metadatas[38]] = ''
        lr[metadatas[39]] = ''

        lr[metadatas[48]] = ''
        lr[metadatas[52]] = ''
        lr[metadatas[53]] = ''
        lr[metadatas[55]] = ''
        lr[metadatas[56]] = ''
        lr[metadatas[57]] = ''
        lr[metadatas[58]] = ''
        lr[metadatas[59]] = ''
        lr[metadatas[60]] = ''
        lr[metadatas[62]] = ''
        ###############################################################################################################
        t2 = time.time()
        LOG.info('解析页面耗时:%f' % (t2-t1))
        #将分析的信息写入数据库
        #www58com.mysql.save(lr, metadatas)
        MySqlEx.save(lr, metadatas)

    #def getinfo(self, urlbase):
        #LOG.info('58getinfo获得%s挂牌详细信息' , urlbase.url)


if __name__ == '__main__':
    wajk = wwwajkcom()
    # wajk.default(UrlBase('http://www.anjuke.com/sy-city.html', 'wwwajkcom',order='123'))
    #wajk.getpages(UrlBean('http://heb.anjuke.com/sale/o5-p2/', 'wwwajkcom#getitem', param=10, headers='哈尔滨',order='123'))
    wajk.getitem(UrlBean('http://shanghai.anjuke.com/prop/view/A423709815', 'wwwajkcom#getitem', param=9, headers='哈尔滨',order='123456'))
    # print(w58.getitem(UrlBean('http://nj.58.com/ershoufang/23200967661449x.shtml?psid=199696111190136559394502435&entinfo=23200967661449_0', 'www58com#getpages', param='北京',order='12345632156')))
#print(www58com.__name__)
#print(www58com.getcitys.__name__)
#print(isinstance([], list))

"""
print(www58com.__name__)
#print(sys.modules.keys())
print(sys.modules['__main__'])

import sys
import os

def get_module():

    def main_module_name():
        mod = sys.modules['__main__']
        file = getattr(mod, '__file__', None)
        return file and os.path.splitext(os.path.basename(file))[0]

    def modname(fvars):

        file, name = fvars.get('__file__'), fvars.get('__name__')
        if file is None or name is None:
            return None

        if name == '__main__':
            name = main_module_name()
        return name

    module_name = modname(globals())
    # print globals()
    # print module_name

get_module()
"""